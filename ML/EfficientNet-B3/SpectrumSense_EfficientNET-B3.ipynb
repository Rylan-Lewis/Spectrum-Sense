{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MAEzGv_dJXL2",
        "outputId": "056b6961-77f9-40e4-e9cb-e4b81da67e10"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8G9svjPQJAFL"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from math import ceil\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.models as models\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1oudJN55Jc9F"
      },
      "outputs": [],
      "source": [
        "base_model = [\n",
        "    #expand_ratio, channels, repeats, stride,kernel_size\n",
        "    [1,16,1,1,3],\n",
        "    [6,24,2,2,3],\n",
        "    [6,40,2,2,5],\n",
        "    [6,80,3,2,3],\n",
        "    [6,112,3,1,5],\n",
        "    [6,192,4,2,5],\n",
        "    [6,320,1,1,3],\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RffCj-nvbHry"
      },
      "outputs": [],
      "source": [
        "phi_values = {\n",
        "    #phi values of all the variations of efficientNet\n",
        "    \"b0\": (0,224,0.2),\n",
        "    \"b1\": (0.5,240,0.2),\n",
        "    \"b2\":(1,260,0.3),\n",
        "    \"b3\":(1,260,0.3),\n",
        "    \"b4\":(3,380,0.4),\n",
        "    \"b5\":(4,456,0.4),\n",
        "    \"b6\":(5,528,0.5),\n",
        "    \"b7\":(6,600,0.5),\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2hYblBSFkEBP"
      },
      "outputs": [],
      "source": [
        "class InitialBlock(nn.Module):\n",
        "  def __init__(self,in_channels, out_channels, kernel_size, stride, padding, groups):\n",
        "    super(InitialBlock,self).__init__()\n",
        "    self.cnn = nn.Conv2d(\n",
        "        in_channels,\n",
        "        out_channels,\n",
        "        kernel_size,\n",
        "        stride,\n",
        "        padding,\n",
        "        groups=groups,\n",
        "        bias=False\n",
        "    )\n",
        "    self.bn = nn.BatchNorm2d(out_channels)\n",
        "    self.silu = nn.SiLU()\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.silu(self.bn(self.cnn(x)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MvH2t2xiJu6L"
      },
      "outputs": [],
      "source": [
        "class SqueezeExcitation(nn.Module):\n",
        "  def __init__(self, in_channels, reduced_dim):\n",
        "    super(SqueezeExcitation , self).__init__()\n",
        "    self.se = nn.Sequential(\n",
        "        nn.AdaptiveAvgPool2d(1), # I/P: CxHxW -> O/P: Cx1x1 (C-> No.of channels , H->Height, W->Width)\n",
        "        nn.Conv2d(in_channels,reduced_dim,1),\n",
        "        nn.SiLU(),\n",
        "        nn.Conv2d(reduced_dim , in_channels,1),\n",
        "        nn.Sigmoid(),\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return x*self.se(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c6TJ9kbdSn_n"
      },
      "outputs": [],
      "source": [
        " # reduction part is for telling the squeeze excitation how much it should reduced dimensionality of the image, survival_prob is for stochastic depth\n",
        "class MB(nn.Module):\n",
        "  def __init__(\n",
        "      self,in_channels,out_channels,kernel_size,stride,padding,expand_ratio,reduction=4 ,survival_prob=0.8\n",
        "  ):\n",
        "    super(MB,self).__init__()\n",
        "    self.survival_prob = survival_prob\n",
        "    self.use_residual = in_channels == out_channels and stride == 1\n",
        "    hidden_dim = in_channels*expand_ratio\n",
        "    self.expand = in_channels !=hidden_dim\n",
        "    reduced_dim = int(in_channels/reduction)\n",
        "\n",
        "    self.conv = nn.Sequential(\n",
        "              # Initial expansion if necessary\n",
        "              InitialBlock(in_channels, hidden_dim, kernel_size=3, stride=1, padding=1, groups=1) if self.expand else nn.Identity(),\n",
        "              # Depthwise convolution\n",
        "              InitialBlock(hidden_dim, hidden_dim, kernel_size, stride, padding, groups=hidden_dim),\n",
        "              # Squeeze-and-Excitation\n",
        "              SqueezeExcitation(hidden_dim, reduced_dim),\n",
        "              # Pointwise convolution\n",
        "              nn.Conv2d(hidden_dim, out_channels, 1, bias=False),\n",
        "              nn.BatchNorm2d(out_channels)\n",
        "          )\n",
        "\n",
        "  def stochastic_depth(self,x):\n",
        "    if not self.training:\n",
        "      return x\n",
        "\n",
        "\n",
        "    binary_tensor = torch.rand(x.shape[0],1,1,1,device=x.device) < self.survival_prob\n",
        "\n",
        "    return torch.div(x,self.survival_prob) * binary_tensor\n",
        "\n",
        "\n",
        "  def forward(self,inputs):\n",
        "    x = self.conv(inputs)\n",
        "\n",
        "\n",
        "    if not hasattr(self, 'conv'):\n",
        "        raise RuntimeError(\"'conv' attribute is not defined in the MB block.\")\n",
        "\n",
        "\n",
        "    if self.use_residual:\n",
        "      return self.stochastic_depth(x) + inputs\n",
        "    else:\n",
        "      return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4_BROEydjH7c"
      },
      "outputs": [],
      "source": [
        "class EfficientNET(nn.Module):\n",
        "  def __init__(self,version,num_classes):\n",
        "    super(EfficientNET, self).__init__()\n",
        "    width_factor,depth_factor,dropout_rate = self.calculate_factors((version))\n",
        "    last_channels = ceil(1280*width_factor)\n",
        "    self.pool = nn.AdaptiveAvgPool2d(1)\n",
        "    self.features = self.create_features(width_factor,depth_factor,last_channels)\n",
        "    self.classifier = nn.Sequential(\n",
        "        nn.Dropout(dropout_rate),\n",
        "        nn.Linear(last_channels,num_classes),\n",
        "        nn.Softmax(dim=1)\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "  def calculate_factors(self,version,alpha=1.2, beta=1.1):\n",
        "    phi,res,drop_rate = phi_values[version]\n",
        "    depth_factor = alpha ** phi\n",
        "    width_factor = beta ** phi\n",
        "    return width_factor,depth_factor,drop_rate\n",
        "\n",
        "  def create_features(self,width_factor,depth_factor,last_channels):\n",
        "    channels = int(32*width_factor)\n",
        "    features = [InitialBlock(3,channels,3,stride=2,padding=1,groups=1)]\n",
        "    in_channels = channels\n",
        "\n",
        "    for expand_ratio, channels,repeats,stride,kernel_size in base_model:\n",
        "      out_channels = 4*ceil(int(channels*width_factor)/4)\n",
        "      layers_repeats = ceil(repeats*depth_factor)\n",
        "\n",
        "      for layer in range(layers_repeats):\n",
        "        features.append(\n",
        "            MB(\n",
        "                in_channels,\n",
        "                out_channels,\n",
        "                expand_ratio=expand_ratio,\n",
        "                stride=stride if layer == 0 else 1,\n",
        "                kernel_size=kernel_size,\n",
        "                padding=kernel_size//2, #in this basically if we have a kernel of 1,we need padding 0 for same dimension image, if kernel is of size 3 then padding should be of order 1 to get same dimensional output image. so her we take the integer divison by 2 meaning if 1//2 then answer is 0, 3//2 then asnwer is 1\n",
        "            )\n",
        "        )\n",
        "        in_channels = out_channels\n",
        "\n",
        "\n",
        "    features.append(\n",
        "        InitialBlock(in_channels,last_channels,kernel_size=1,stride=1,padding=0,groups=1) #for the last conv layer\n",
        "    )\n",
        "\n",
        "    return nn.Sequential(*features)\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.pool(self.features(x))\n",
        "    return self.classifier(x.view(x.shape[0],-1))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "z3Kb0zU0BcnY",
        "outputId": "b69e87a2-9c02-42c5-a8f3-4ef3bdc3e7ee"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-c7d3a3ac57de>\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m                 \u001b[0;31m# Load the image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0mimg_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m                 \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                 \u001b[0;31m# Apply the data augmentation transforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3234\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3236\u001b[0;31m     \u001b[0mprefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3238\u001b[0m     \u001b[0mpreinit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "import torchvision.transforms.functional as TF\n",
        "\n",
        "# Define the path to your original dataset\n",
        "original_data_dir = '/content/drive/MyDrive/autism/consolidated'\n",
        "\n",
        "# Define the path to the new folder where you want to save the augmented images\n",
        "augmented_data_dir = '/content/drive/MyDrive/autism/augmented'\n",
        "\n",
        "# Create the new folder if it doesn't exist\n",
        "if not os.path.exists(augmented_data_dir):\n",
        "    os.makedirs(augmented_data_dir)\n",
        "\n",
        "# Define the data augmentation transforms\n",
        "data_transforms = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(300), # Adjusted to 300x300\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Iterate over the subfolders in the original dataset\n",
        "for class_name in os.listdir(original_data_dir):\n",
        "    class_dir = os.path.join(original_data_dir, class_name)\n",
        "    if os.path.isdir(class_dir):\n",
        "        # Create a corresponding subfolder in the augmented directory\n",
        "        augmented_class_dir = os.path.join(augmented_data_dir, class_name)\n",
        "        if not os.path.exists(augmented_class_dir):\n",
        "            os.makedirs(augmented_class_dir)\n",
        "\n",
        "        # Iterate over the images in the class subfolder\n",
        "        for filename in os.listdir(class_dir):\n",
        "            if filename.endswith(\".jpg\") or filename.endswith(\".png\"): # Check if the file is an image\n",
        "                # Load the image\n",
        "                img_path = os.path.join(class_dir, filename)\n",
        "                img = Image.open(img_path)\n",
        "\n",
        "                # Apply the data augmentation transforms\n",
        "                augmented_img = data_transforms(img)\n",
        "\n",
        "                # Convert the tensor back to a PIL Image\n",
        "                augmented_img_pil = TF.to_pil_image(augmented_img)\n",
        "\n",
        "                # Save the augmented image to the corresponding subfolder in the new folder\n",
        "                augmented_img_path = os.path.join(augmented_class_dir, filename)\n",
        "                augmented_img_pil.save(augmented_img_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UB9PXRdB73pf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import time\n",
        "\n",
        "# Define the path to the combined dataset folder\n",
        "combined_data_dir = '/content/drive/MyDrive/autism/combined_dataset'\n",
        "\n",
        "# Create the combined dataset folder if it doesn't exist\n",
        "if not os.path.exists(combined_data_dir):\n",
        "    os.makedirs(combined_data_dir)\n",
        "\n",
        "# Copy the original images to the combined dataset folder\n",
        "for class_name in os.listdir(original_data_dir):\n",
        "    class_dir = os.path.join(original_data_dir, class_name)\n",
        "    if os.path.isdir(class_dir):\n",
        "        # Create a corresponding subfolder in the combined dataset folder\n",
        "        combined_class_dir = os.path.join(combined_data_dir, class_name)\n",
        "        if not os.path.exists(combined_class_dir):\n",
        "            os.makedirs(combined_class_dir)\n",
        "\n",
        "        # Copy the images from the original class subfolder to the combined class subfolder\n",
        "        for filename in os.listdir(class_dir):\n",
        "            if filename.endswith(\".jpg\") or filename.endswith(\".png\"): # Check if the file is an image\n",
        "                src_path = os.path.join(class_dir, filename)\n",
        "                dst_path = os.path.join(combined_class_dir, filename)\n",
        "                shutil.copy(src_path, dst_path)\n",
        "\n",
        "# Add the augmented images to the combined dataset folder\n",
        "for class_name in os.listdir(augmented_data_dir):\n",
        "    augmented_class_dir = os.path.join(augmented_data_dir, class_name)\n",
        "    if os.path.isdir(augmented_class_dir):\n",
        "        # Find the corresponding subfolder in the combined dataset folder\n",
        "        combined_class_dir = os.path.join(combined_data_dir, class_name)\n",
        "\n",
        "        # Copy the augmented images to the combined class subfolder\n",
        "        for filename in os.listdir(augmented_class_dir):\n",
        "            if filename.endswith(\".jpg\") or filename.endswith(\".png\"): # Check if the file is an image\n",
        "                src_path = os.path.join(augmented_class_dir, filename)\n",
        "                dst_path = os.path.join(combined_class_dir, filename)\n",
        "                # Check if the file already exists in the destination\n",
        "                if os.path.exists(dst_path):\n",
        "                    # Append a unique identifier to the filename\n",
        "                    base, ext = os.path.splitext(filename)\n",
        "                    new_filename = f\"{base}_{int(time.time())}\" + ext\n",
        "                    dst_path = os.path.join(combined_class_dir, new_filename)\n",
        "                shutil.copy(src_path, dst_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RipIGZ-cMYmW"
      },
      "outputs": [],
      "source": [
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "\n",
        "data_transforms_eff = transforms.Compose([\n",
        "    transforms.Resize((300, 300)), # Resize images to exactly 300x300 pixels\n",
        "    transforms.CenterCrop(300), # Crop the center of the image to 300x300 pixels\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "combined_data_dir = '/content/drive/MyDrive/MP_Dataset/autism/combined_dataset'\n",
        "# Load the augmented data into a dataset\n",
        "combined_dataset = ImageFolder(combined_data_dir, transform=data_transforms_eff)\n",
        "\n",
        "# Define the lengths of the training and validation sets\n",
        "train_size = len(combined_dataset)\n",
        "validation_size = int(train_size * 0.2) # 20% of the data for validation\n",
        "train_size = train_size - validation_size # Adjust the training size\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "train_dataset, validation_dataset = random_split(combined_dataset, [train_size, validation_size])\n",
        "\n",
        "# Create DataLoaders for the training and validation sets\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=2)\n",
        "validation_dataloader = DataLoader(validation_dataset, batch_size=8, shuffle=False, num_workers=2)\n",
        "\n",
        "# Update the dataset_sizes dictionary to include the validation set\n",
        "dataset_sizes = {\n",
        "    'train': train_size,\n",
        "    'validation': validation_size\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 584
        },
        "id": "OsrovVs5G4Ao",
        "outputId": "737e885c-1cf6-42fb-9fbd-8a8e85808620"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 55/100\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train Loss: 0.4544 Acc: 0.8524\n",
            "validation Loss: 0.4238 Acc: 0.8838\n",
            "Epoch 56/100\n",
            "----------\n",
            "train Loss: 0.4608 Acc: 0.8460\n",
            "validation Loss: 0.4246 Acc: 0.8846\n",
            "Epoch 57/100\n",
            "----------\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-683a2860da96>\u001b[0m in \u001b[0;36m<cell line: 33>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mphase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m                     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m                     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m             )\n\u001b[0;32m--> 522\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    523\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.models as models\n",
        "import os\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_save_path = '/content/drive/MyDrive/MP_Dataset/SpectrumSense/Model_checkpoints/EfficientNET/Efficientnet_b3_100epochs.pth'\n",
        "checkpoint_exists = os.path.isfile(model_save_path)\n",
        "\n",
        "# Initializing the model\n",
        "model = EfficientNET('b3',2).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Load the checkpoint if it exists\n",
        "if checkpoint_exists:\n",
        "    checkpoint = torch.load(model_save_path,map_location=torch.device('cpu'))\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    last_epoch = checkpoint['epoch']\n",
        "    last_loss = checkpoint['loss']\n",
        "else:\n",
        "    # If the checkpoint does not exist, initialize the last epoch and loss\n",
        "    last_epoch = 0\n",
        "    last_loss=0.0\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "num_epochs = 100\n",
        "\n",
        "for epoch in range(last_epoch + 1, num_epochs+1):\n",
        "    print('Epoch {}/{}'.format(epoch, num_epochs))\n",
        "    print('-' * 10)\n",
        "\n",
        "    for phase in ['train', 'validation']:\n",
        "        if phase == 'train':\n",
        "            model.train()\n",
        "        else:\n",
        "            model.eval()\n",
        "\n",
        "        running_loss = 0.0\n",
        "        running_corrects = 0\n",
        "\n",
        "        # Use the appropriate DataLoader based on the phase\n",
        "        if phase == 'train':\n",
        "            dataloader = train_dataloader\n",
        "        else:\n",
        "            dataloader = validation_dataloader\n",
        "\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            with torch.set_grad_enabled(phase == 'train'):\n",
        "                outputs = model(inputs)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                if phase == 'train':\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "        epoch_loss = running_loss / dataset_sizes[phase]\n",
        "        epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "        print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
        "\n",
        "    # Save the model state after each epoch\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'loss': epoch_loss,\n",
        "        'accuracy': epoch_acc,\n",
        "    }, model_save_path)\n",
        "\n",
        "print('Training complete')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 758
        },
        "id": "eF4zbw2wz0VF",
        "outputId": "f6c22dc9-e326-40db-c3f6-e0d948ccc366"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Precision: 0.7959183673469388\n",
            "Recall: 0.78\n",
            "F1-score: 0.7878787878787878\n",
            "Accuracy: 0.79\n",
            "Confusion Matrix:\n",
            "Specificity: 0.8\n",
            "[[80 20]\n",
            " [22 78]]\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x600 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Define the transforms for evaluation\n",
        "data_transforms_res = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize images to 224x224 pixels\n",
        "    transforms.CenterCrop(224),      # Center crop\n",
        "    transforms.ToTensor(),           # Convert to tensor\n",
        "])\n",
        "\n",
        "# Load the dataset for evaluation\n",
        "eval_dataset = ImageFolder('/content/drive/MyDrive/MP_Dataset/autism/test', transform=data_transforms_res)\n",
        "\n",
        "# Create DataLoader for evaluation\n",
        "eval_dataloader = DataLoader(eval_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "# Load the MobileNetV3 model\n",
        "model = EfficientNET('b3',2).to(device)\n",
        "checkpoint = torch.load('/content/drive/MyDrive/MP_Dataset/SpectrumSense/Model_checkpoints/EfficientNET/Efficientnet_b3_70epochs_wr.pth', map_location=torch.device('cpu'))\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "# Initialize lists to store predictions and ground truth labels\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "# Iterate over the evaluation dataset\n",
        "for inputs, labels in eval_dataloader:\n",
        "    inputs = inputs.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    # Forward pass\n",
        "    with torch.no_grad():\n",
        "        outputs = model(inputs)\n",
        "\n",
        "    preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
        "    all_preds.extend(preds)\n",
        "    all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "precision = precision_score(all_labels, all_preds)\n",
        "recall = recall_score(all_labels, all_preds)\n",
        "f1 = f1_score(all_labels, all_preds)\n",
        "accuracy = accuracy_score(all_labels, all_preds)\n",
        "conf_matrix = confusion_matrix(all_labels, all_preds)\n",
        "specificity = conf_matrix[0, 0] / (conf_matrix[0, 0] + conf_matrix[0, 1])\n",
        "\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-score:\", f1)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(\"Specificity:\", specificity)\n",
        "print(conf_matrix)\n",
        "classes = eval_dataset.classes\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xticks(rotation=45)\n",
        "plt.yticks(rotation=0)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "AJywevgZMCbT",
        "outputId": "e7f94eed-6e55-457b-c1b5-145c9f686104"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-e8311c39e84d>\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEfficientNET\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'b3'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_save_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Set the model to evaluation mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1024\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpicklingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mUNSAFE_MESSAGE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m                 return _load(opened_zipfile,\n\u001b[0m\u001b[1;32m   1027\u001b[0m                              \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m                              \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1436\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnpicklerWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1437\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1438\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1440\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_loaded_sparse_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mpersistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1407\u001b[0m             \u001b[0mnbytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumel\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_element_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1408\u001b[0;31m             \u001b[0mtyped_storage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_maybe_decode_ascii\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1410\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtyped_storage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   1380\u001b[0m         \u001b[0;31m# stop wrapping with TypedStorage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1381\u001b[0m         typed_storage = torch.storage.TypedStorage(\n\u001b[0;32m-> 1382\u001b[0;31m             \u001b[0mwrap_storage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrestore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1383\u001b[0m             \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m             _internal=True)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdefault_restore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_package_registry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_cuda_deserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m         \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_cuda_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_torch_load_uninitialized\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mvalidate_cuda_device\u001b[0;34m(location)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m         raise RuntimeError('Attempting to deserialize object on a CUDA '\n\u001b[0m\u001b[1;32m    251\u001b[0m                            \u001b[0;34m'device but torch.cuda.is_available() is False. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m                            \u001b[0;34m'If you are running on a CPU-only machine, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import os\n",
        "import torch.nn.functional as F\n",
        "\n",
        "model_save_path = '/content/drive/MyDrive/MP_Dataset/SpectrumSense/Model_checkpoints/EfficientNET/Efficientnet_b3_70epochs_wr.pth'\n",
        "\n",
        "# Path to the test image\n",
        "test_image_path = '/content/drive/MyDrive/MP_Dataset/autism/test/non_autistic/050.jpg'\n",
        "\n",
        "\n",
        "data_transforms_test = transforms.Compose([\n",
        "    transforms.Resize((300, 300)),\n",
        "    transforms.CenterCrop(300),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "\n",
        "model = EfficientNET('b3',2)\n",
        "checkpoint = torch.load(model_save_path)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model.eval() # Set the model to evaluation mode\n",
        "model.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Load and transform the test image\n",
        "image = Image.open(test_image_path)\n",
        "image = data_transforms_test(image).unsqueeze(0)\n",
        "\n",
        "image = image.to(device)\n",
        "\n",
        "# Make a prediction\n",
        "with torch.no_grad():\n",
        "    outputs = model(image)\n",
        "    probabilities = F.softmax(outputs, dim=1)\n",
        "    _, preds = torch.max(probabilities, 1)\n",
        "\n",
        "class_names = ['Not Autistic', 'Autistic']\n",
        "\n",
        "# Predicted class name\n",
        "predicted_class = class_names[preds.item()]\n",
        "\n",
        "# Print the probabilities of confidence rate for each class\n",
        "print(f'Predicted class: {predicted_class}')\n",
        "print(f'Probabilities: {probabilities.tolist()}')\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
